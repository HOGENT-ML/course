{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 7 – Ensemble Learning and Random Forests**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/HOGENT-ML/course/blob/main/710-ensemble_learning_and_random_forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.7 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 7)\n",
    "\n",
    "# Scikit-Learn ≥1.01 is required\n",
    "from packaging import version\n",
    "import sklearn\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It can be proven that if you aggregate (e.g. by taking the average prediction) the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor.\n",
    "- A group of predictors is called an _ensemble_.\n",
    "- This technique is called _Ensemble Learning_.\n",
    "- An Ensemble Learning algorithm is called an _Ensemble method_.\n",
    "\n",
    "Example: \n",
    "- Train a group of Decision Tree classifiers, each on a different random subset of the training set. \n",
    "- To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes. \n",
    "- Such an ensemble of Decision Trees is called a _Random Forest_.\n",
    "- This is one of the most powerful Machine Learning algorithms available today."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers\n",
    "\n",
    "Typically you choose several predictors to take part in the ensemble:\n",
    "\n",
    "![](img/ensemble_diverse_predictors.png)\n",
    "\n",
    "**Hard voting**\n",
    "\n",
    "In _hard voting_ we predict the class that gets the most votes:\n",
    "\n",
    "![](img/ensemble_hard_vorting.png)\n",
    "\n",
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the moons dataset to illustrate this and build a voting classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you fit a VotingClassifier, it clones every estimator and fits the clones. The\n",
    "original estimators are available via the `estimators` attribute, while the fitted clones are available via the `estimators_` attribute. If you prefer a dict rather than a list, you can use `named_estimators` or `named_estimators_` instead.   \n",
    "\n",
    "To begin, let’s look at each fitted classifier’s accuracy on the test set which is returned by de `score()` method of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the voting classifier’s `predict()` method, it performs hard voting. For\n",
    "example, the voting classifier predicts class 1 for the first instance of the test set, because two out of three classifiers predict that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1], dtype=int64), array([1], dtype=int64), array([0], dtype=int64)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the performance of the voting classifier on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it! The voting classifier outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Soft voting**\n",
    "- If all classifiers are able to estimate class probabilities (i.e., they all have a `predict_proba()` method), then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. \n",
    "- This is called `soft voting`. \n",
    "- It often achieves higher performance than hard voting because it gives more weight to highly confident votes.\n",
    "\n",
    "All you have to do is pass the parameter `voting=soft` to the `VotingClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach 92% accuracy simply by using soft voting—not bad!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "There are 2 ways to use a diverse set of classifiers:\n",
    "1. use very different training algorithms (see above - Voting)\n",
    "1. use the same training algorithm for every predictor and train them on different random subsets of the training set\n",
    "\n",
    "**Bagging** and **pasting** is about this second way and refer to how this subset is chosen: \n",
    "\n",
    "- _Bagging_: sampling _with replacement_: each training sample can appear multiple times when training a single predictor.   \n",
    "  &rarr; Imagine picking a card randomly from a deck of cards, writing it down, then placing it back in the deck\n",
    "before picking the next card: the same card could be sampled multiple times.\n",
    "- _Pasting_: sampling _without replacement_\n",
    "\n",
    "(Bagging is short for _bootstrap aggregating_, which is the term used in statistics.)\n",
    "  \n",
    "Conclusion: \n",
    "\n",
    "- both bagging and pasting allow training instances to be sampled several times across multiple predictors.\n",
    "- only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "![](img/bagging-pasting.png) \n",
    "- The predictors in the above diagram are e.g. 4 Decision Tree Classfiers that are each trained with another subset of the training data.  \n",
    "- Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors.\n",
    "  &rarr; Most frequent predictor (like hard voting) for classification and average for regression.\n",
    "- Each individual predictor has a higher bias than if it were trained on the original training set (because it has fewer data). \n",
    "- Aggregation reduces both bias and variance.\n",
    "- Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
    "- Predictors can all be trained in parallel, via different CPU cores or even different servers --> bagging and pasting scale very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in Scikit-Learn\n",
    "Use `BaggingClassifier` class or `BaggingRegressor` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1,random_state=42)  # n_jobs: number of CPU cores used for training and prediction (-1 = all available)\n",
    "# our training data is still the moons dataset with 500 instances\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "The `BaggingClassifier` automatically performs soft voting instead of hard voting if the base\n",
    "classifier can estimate class probabilities (i.e., if it has a `predict_proba()` method), which is the case\n",
    "with Decision Tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to a single decison tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856\n"
     ]
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The figure below compares the decision boundary of a single decision tree with the decision boundary of a bagging ensemble of 500 trees (from the preceding code), both trained on the moons dataset.   \n",
    "- As we can see, the ensemble’s predictions will likely generalize much better (less overfitting!) than the single decision tree’s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular).\n",
    "![](img/dt_bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. \n",
    "- By default (parameter `max_samples` is omitted) a `BaggingClassifier` samples $m$ training instances with replacement (`bootstrap=True`), where $m$ is the size of the training set. \n",
    "- So in a training set of 100 samples we sample 100 times a single sample, but since each sample is replaced after sampling we have many duplicates. \n",
    "- With this process, it can be shown mathematically that only about 63% of the training instances are sampled on average for each predictor. \n",
    "- The remaining 37% of the training instances that are not sampled are called _out-of-bag_ (oob) instances. \n",
    "- Note that they are not the same 37% for all predictors.\n",
    "- Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set. \n",
    "- Indeed, if there are enough estimators, then each instance in the training set will likely be an OOB instance of several estimators, so these estimators can be used to make a fair ensemble prediction for that instance.\n",
    "- You can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "- The resulting evaluation score is available in the `oob_score_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, oob_score=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.3375    , 0.6625    ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_[:3]  # probas for the first 3 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this with the accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 92% accuracy on the test. The OOB evaluation was a bit too pessimistic, just over 2% too low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest is an ensemble of decision trees, generally trained via the bagging method (or sometimes pasting) \n",
    "- Typically with `max_samples` set to the size of the training set. \n",
    "- Instead of building a B`aggingClassifier` and passing it a `DecisionTreeClassifier`, you can use the `RandomForestClassifier` class, which is more convenient and optimized for decision trees\n",
    "- Similarly, there is a `RandomForestRegressor` class for regression tasks. \n",
    "- The following code trains a random forest classifier with 500 trees, each limited to maximum 16 leaf nodes, using all available CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, a `RandomForestClassifier` has all the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Random Forest is equivalent to a bag of decision trees.\n",
    "- By default, it samples $\\sqrt{n}$ features (where n is the total number of features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
    "    n_estimators=500, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – verifies that the predictions are identical\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "np.all(y_pred_bag == y_pred_rf)  # same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Yet another great quality of Random Forests is that they make it easy to **measure the relative importance of each feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 sepal length (cm)\n",
      "0.02 sepal width (cm)\n",
      "0.44 petal length (cm)\n",
      "0.42 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score, 2), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11249225, 0.02311929, 0.44103046, 0.423358  ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The petal length and width are clearly much more important to determine the iris species than the sepal data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the features (= pixel) importance for the MNIST dataset:  \n",
    "\n",
    "![](img/mnist_pixel_importance.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "- Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. \n",
    "- The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. \n",
    "- There are many boosting methods available, but by far the most popular are _AdaBoost_ (short for _Adaptive Boosting_) and _Gradient Boosting_. \n",
    "- Let’s start with AdaBoost.\n",
    "## AdaBoost\n",
    "- One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. \n",
    "- This results in new predictors focusing more and more on the hard cases.\n",
    "- The algorithm first trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set.\n",
    "- The algorithm then increases the relative weight of misclassified training instances. \n",
    "- Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/adaboost.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn uses a multiclass version of AdaBoost called _SAMME_ (which stands for **Stagewise Additive Modeling using a Multiclass Exponential loss function**).\n",
    "- If the predictors can estimate class probabilities (i.e., if they have a `predict_proba()` method), Scikit-Learn can use a variant of SAMME called SAMME.R.\n",
    "- The R stands for “Real”, which relies on class probabilities rather than predictions and generally performs better.\n",
    "- The learning rate determines the weights of each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the `learning_rate` and `n_estimators` parameters. Values must be in the range (0.0, inf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train) # we are still using the moons dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. \n",
    "- However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let create a simple, noisy quadratic dataset and fit a `DecisionTreeRegressor` to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll train a second DecisionTreeRegressor on the residual errors made by the first predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=43)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train a third regressor on the residual errors made by the second predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=44)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49484029, 0.04021166, 0.75026781])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The figure below represents the predictions of these three trees in the left column, and the ensemble’s predictions in the right column. \n",
    "- In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. \n",
    "- In the second row, a new tree is trained on the residual errors of the first tree. \n",
    "- On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. \n",
    "- Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble.\n",
    "![](img/gradient_boosting_plot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a gradient boosting regressor. The following code creates the same ensemble as the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
    "                                 learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `learning_rate` hyperparameter scales the contribution of each tree. \n",
    "- If you set it to a low value, such as 0.1, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. \n",
    "- This is a regularization technique called `shrinkage`.\n",
    "\n",
    "- The figure below shows two GBRT ensembles: \n",
    "    - the one on the left is based on the exmaple above and does not have enough trees to fit the training set\n",
    "    - the one on the right has too many trees and overfits the training set.\n",
    "\n",
    "![](img/gbrt_learning_rate_plot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting with Early stopping**\n",
    "\n",
    "- To find the optimal number of trees, you could perform cross-validation using GridSearchCV, as usual, but there’s a simpler way: \n",
    "    - if you set the `n_iter_no_change` hyperparameter to an integer value, say 10, then the Gradient\n",
    "BoostingRegressor will automatically stop adding more trees during training if it sees that the last 10 trees didn’t help. \n",
    "- Let’s train the ensemble using early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.05, max_depth=2, n_estimators=500,\n",
       "                          n_iter_no_change=10, random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
    "    n_iter_no_change=10, random_state=42)\n",
    "gbrt_best.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.n_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation errors are represented on the left of the figure, and the best model’s predictions are represented on the right.\n",
    "![](img/early_stopping_gbrt_plot.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking   \n",
    "  \n",
    "- The last ensemble method we will discuss in this chapter is called stacking (short for\n",
    "stacked generalization).\n",
    "- It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to perform this aggregation? \n",
    "- The figure below shows such an ensemble performing a regression task on a new instance. \n",
    "Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
    "(called a blender, or a meta learner) takes these predictions as inputs and makes the final prediction (3.0).\n",
    "\n",
    "![](img/stacking1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To train the blender, you first need to build the blending training set. \n",
    "- You can use `cross_val_predict()` on every predictor in the ensemble to get predictions for each instance in the original training set and use these as the input features to train the blender\n",
    "- The targets can simply be copied from the original training set. \n",
    "- Note that regardless of the number of features in the original training set (just one in this example), the blending training set will contain one input feature per predictor (three in this example). \n",
    "\n",
    "![](img/stacking2.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides two classes for stacking ensembles: `StackingClassifier` and\n",
    "`StackingRegressor`. \n",
    " \n",
    "For example, we can replace the `VotingClassifier` we used at\n",
    "the beginning of this chapter on the moons dataset with a StackingClassifier:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                               ('rf', RandomForestClassifier(random_state=42)),\n",
       "                               ('svc', SVC(probability=True, random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(random_state=43))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5  # number of cross-validation folds\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.928"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_clf.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate this stacking model on the test set, we find 92.8% accuracy, which is a bit better than the voting classifier using soft voting, which got 92%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Conclusion**:   \n",
    "> Ensemble methods are versatile, powerful, and fairly simple to use.  \n",
    ">  \n",
    "> **Random forests, AdaBoost, and GBRT** are among **the first models you should test** for most machine learning tasks, and they particularly shine with heterogeneous tabular data. Moreover, as they require very little preprocessing, they’re great for getting aprototype up and running quickly.   \n",
    ">\n",
    "> Lastly, **ensemble methods** like voting classifiers and stacking classifiers **can help push your system’s performance** to its limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "nav_menu": {
   "height": "252px",
   "width": "333px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "06695702c909af4d2de424dd0ca7f59dcd2ee8fd94cb47bf2545198e411600f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
