{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 4 – Training Models**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far we have treated Machine Learning models and their training algorithms mostly like black boxes.\n",
    "- Having a good understanding of how things work can help you pick the appropriate model, the right training algorithm, and provide you with a good set of hyperparameters for your task.\n",
    "- Most of the topics discussed in this chapter will be essential in understanding, building, and training neural networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Content***\n",
    "1. Linear Regression\n",
    "2. Polynomial Regression.\n",
    "3. Regularized Linear Models\n",
    "4. Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.7 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥1.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.11 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 11)\n",
    "\n",
    "# Scikit-Learn ≥1.01 is required\n",
    "from packaging import version\n",
    "import sklearn\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n",
    "\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chapter 1 we looked at a simple regression model of life satisfaction:\n",
    "\n",
    "$$\n",
    "LifeSatisfaction = θ_0 + θ_1 × GDPPerCapita\n",
    "$$\n",
    "\n",
    "More generally: \n",
    "\n",
    "$$\n",
    "\\hat{y} = θ_0 + θ_1x_1 + θ_2x_2 +⋯+ θ_nx_n\n",
    "$$\n",
    "\n",
    "In this equation: \n",
    "\n",
    "- $\\hat{y}$ is the predicted value.\n",
    "- $n$ is the number of features.\n",
    "- $x_i$ is the i<sup>th</sup> feature value.\n",
    "- $\\theta_j$ is the j<sup>th</sup> model parameter (including the bias term θ<sub>0</sub> and the feature weights θ<sub>1</sub>, θ<sub>2</sub>, ⋯, θ<sub>n</sub>).\n",
    "\n",
    "This can be written much more concisely using a vectorized form as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_θ(\\mathbf{x})  = \\mathbf{θ} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "In this equation:\n",
    "- $\\mathbf{θ}$ is the model’s parameter vector, containing the bias term _θ<sub>0</sub>_ and the feature weights _θ<sub>1</sub>_ to _θ<sub>n</sub>_.\n",
    "- $\\mathbf{x}$ is the instance’s feature vector, containing x<sub>0</sub> to x<sub>n</sub> , with x<sub>0</sub> always equal to 1.\n",
    "- $\\mathbf{θ} \\cdot \\mathbf{x}$ is the dot product of the vectors $\\mathbf{θ}$ and $\\mathbf{x}$, which is equal to $θ_0x_0 + θ_1x_1 + θ_2x_2 +⋯+ θ_nx_n$.\n",
    "- $h$ is the hypothesis function, using the model parameters $\\mathbf{θ}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, vectors are often represented as column vectors, which are 2D\n",
    "arrays with a single column. If $\\mathbf{θ}$ and $\\mathbf{x}$ are column vectors, then the prediction is\n",
    "$\\hat{y} = \\mathbf{θ}^T\\mathbf{x}$, where $\\mathbf{θ}^T$ is the transpose of $\\mathbf{θ}$ (a row vector instead of a column vector) and\n",
    "$\\mathbf{θ}^T\\mathbf{x}$ is the matrix multiplication of $\\mathbf{θ}^T$ and $\\mathbf{x}$. It is of course the same prediction, except\n",
    "that it is now represented as a single-cell matrix rather than a scalar value. In this course we\n",
    "will use this notation to avoid switching between dot products and matrix\n",
    "multiplications.  \n",
    "\n",
    "To train a Linear Regression model, we need to find the value of $\\mathbf{θ}$ that minimizes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root). \n",
    "\n",
    "The MSE of a Linear Regression hypothesis $h_θ$ on a training set $\\mathbf{X}$ is calculated using the equation \n",
    "\n",
    "$$\n",
    "MSE(\\mathbf{X},h_θ) = \\frac{1}{m}\\sum_{i=1}^{m}(\\mathbf{θ}^T \\mathbf{x}^{(i)} - {y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "with $m$ the number of instances in the training set. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some linear-looking data to test the algorithms on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1) # 100 uniform random numbers in [0, 2]\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # Gaussian noise: N(0, 1): mean = 0, std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Regression using Scikit-Learn is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to 3*x + 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to visualize the fitted line: just predict two x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "y_predict = lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "# extra code – beautifies and saves Figure 4–2\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` function (the name stands for \"least squares\", referring to MSE).  \n",
    "Please note that you could also use the SVD decomposition or the pseudoinverse (see course: Mathematics for Machine Learning)  \n",
    "Calculate $\\mathbf{X}^+\\mathbf{y}$, where $\\mathbf{X}^{+}$ is the _pseudoinverse_ of $\\mathbf{X}$ (specifically the Moore-Penrose inverse).  \n",
    "\n",
    "To be able to do so, you need to extend the X with a dummy variable X_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "X_b = add_dummy_feature(X)  # add x0 = 1 to each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "The approach used by Scikit-Learn’s LinearRegression class is about $O(n^2)$. **If you double the number of features, you multiply the computation time by roughly 4**. It gets very slow when the number of features grows large (e.g., 100,000). On the positive side, **it is linear with regard to the number of instances** in the training set ($O(m)$), so it handles large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "Also, **once you have trained your Linear Regression model, predictions are very fast: the computational complexity is linear with regard to both the number of\n",
    "instances you want to make predictions on and the number of features**. In other words, making predictions on twice as many instances (or twice as\n",
    "many features) will take roughly twice as much time. \n",
    "\n",
    "Now we will look at a very different way to train a Linear Regression model, which is better suited for cases where there are a large number of features or\n",
    "too many training instances to fit in memory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "_Gradient Descent_ is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient\n",
    "Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "Concretely, you start by filling $\\mathbf{θ}$ with random values (this is called random initialization). Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum. The learning step size is proportional to\n",
    "the slope of the cost function, so the steps gradually get smaller as the parameters approach the\n",
    "minimum\n",
    "\n",
    "![](img/gradient_descent.png)\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determined by the _learning rate_ hyperparameter:  \n",
    "- If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.\n",
    "- If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before.\n",
    "\n",
    "\n",
    "## Batch Gradient Descent\n",
    "\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost function with regard to each model parameter $θ_j$. \n",
    "- In other words, you need to calculate how much the cost function will change if you change $θ_j$ just a little bit.\n",
    "- This is called a _partial derivative_. It is like asking “What is the slope of the mountain under my feet if I face east?” and then asking the\n",
    "same question facing north (and so on for all other dimensions). \n",
    "\n",
    "Notice that this approach involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: \n",
    "- It uses the whole batch of training data at every step. \n",
    "- As a result it is terribly slow on very large training sets.\n",
    "\n",
    "Let’s look at a quick implementation of this algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # number of instances\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # randomly initialized model parameters\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)  # vectorized form of the gradient equation (4-5) in the book (@ is matrix multiplication)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this is the exact same solution as the linear regression found. \n",
    "Look at the next figure to see what the effect is of a different learning rate. The figure shows the first 20 steps of Gradient Descent using three different learning rates.  \n",
    "The line at the bottom of each plot represents the random starting point, then each epoch is represented by a darker and darker line.\n",
    "![](img/gradient_descent_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome the problem of training the whole batch at every step, _Stochastic Gradient Descent_ picks a random instance in the training set at every step\n",
    "and computes the gradients based only on that single instance.\n",
    "\n",
    "- This makes the algorithm much faster because it has very little data to manipulate at every iteration.\n",
    "- It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration. \n",
    "- On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently\n",
    "decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average:\n",
    "\n",
    "![](img/sgd.png)\n",
    "\n",
    "In practice, with stochastic gradient descent we go through a number of rounds (called _epochs_). \n",
    "- In each epoch we iterate through $m$ (= number of instances in the training set) randomly selected instances. \n",
    "- At each such an iteration we calculate the gradient and try to approach the minimum a bit closer. \n",
    "- So we have a nested loop:\n",
    "```\n",
    "  for epoch in range(n_epochs):  \n",
    "    for iteration in range(m):\n",
    "    # calculate gradient for a randomly (stochastic) chosen instance\n",
    "```\n",
    "\n",
    "**The number of epochs is an important hyperparameter for algorithms that use stochastic gradient descent.**\n",
    "\n",
    "To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the `SGDRegressor` class, which defaults to optimizing the squared\n",
    "error cost function. The following code runs for maximum 1.000 epochs or until the loss drops by less than 0.00001 during one epoch (`max_iter=1000,\n",
    "tol=1e-5`). It starts with a learning rate of 0.01 (`eta0=0.01`). Lastly, it does not use\n",
    "any regularization (penalty=None; more details on this shortly):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y.ravel()[:5] # ravel() returns a flattened array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,n_iter_no_change=100, random_state=42)\n",
    "# n_iter_no_change=100 means that the training will stop after 100 epochs without improvement\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets (not 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to the result of the linear regression above.  \n",
    "The resulting image below shows the first 20 steps of training, illustrating the irregularity in the steps in the process.\n",
    "<img src=\"img/sgd_plot.png\" width=\"50%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "The last Gradient Descent algorithm we will look at is called Mini-batch Gradient\n",
    "Descent. It is the compromise between Batch and Stochastic Gradient Descent:\n",
    "at each step, instead of computing the gradients based on the full training set (as\n",
    "in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD\n",
    "computes the gradients on small random sets of instances called mini-batches. The\n",
    "main advantage of Mini-batch GD over Stochastic GD is that you can get a performance\n",
    "boost from hardware optimization of matrix operations, especially when using\n",
    "Graphical Processing Units (GPUs).\n",
    "<img src=\"img/gradient_descent_paths_plot.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Polynomial Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if your data is more complex than a straight line? \n",
    "- You can use a linear model to fit nonlinear data\n",
    "- Add powers of each feature as new features\n",
    "- Train a linear model on this extended set of features.  \n",
    "This technique is called _Polynomial Regression_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# include_bias=False means that the PolynomialFeatures class will not add the bias term (x0 = 1) for us\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_poly` now contains the original feature of X plus the square of this feature. Now you can fit a `LinearRegression` model to this extended training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad: the model estimates $\\hat{y} = 0.56x_1^2 + 0.93x_1 + 1.78$ when in fact the original function was $y = 0.5x_1^2 + 1.0x_1 + 2.0$ + Gaussian noise. \n",
    "\n",
    "Note that when there are multiple features (e.g. $x_1$ and $x_2$), Polynomial Regression is capable of finding relationships between features (which is something a plain Linear Regression model cannot do). This is made possible by the fact that `PolynomialFeatures` also adds all combinations of features up to the given\n",
    "degree. For example, if there were two features $a$ and $b$, `PolynomialFeatures` with degree=3 would not only add the features $a^2$, $a^3$, $b^2$ and $b^3$, but also the combinations $ab$, $a^2 b$, and $ab^2$.\n",
    "\n",
    "High-degree Polynomial Regression can fit the training data much better than plain Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The high-degree Polynomial Regression model is severely overfitting the training data.\n",
    "- The linear model is underfitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you perform high-degree Polynomial Regression, you will likely fit the training data much better than with plain Linear Regression.  \n",
    "Take a look at the figure below where we fit a 300-degree polynomial, a quadratic model (second degree) and a pure linear model (first degree) through the training set.  \n",
    "Notice how the 300-degree polynomial model wiggles around to get as close as possible to the training instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for style, width, degree in ((\"r-+\", 2, 1), (\"b--\", 2, 2), (\"g-\", 1, 300)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    label = f\"{degree} degree{'s' if degree > 1 else ''}\"\n",
    "    plt.plot(X_new, y_newbig, style, label=label, linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high-degree Polynomial Regression model is severely overfitting the training data, while the linear model is underfitting it.  \n",
    "- Quadratic model generalizes best (makes sense: data generated using a quadratic model)  \n",
    "- Normally, this information is not there, so how complex should the model be?  \n",
    "- How to know when model is overfitting or underfitting?  \n",
    "=> Chapter 2: cross-validation!!!  \n",
    "- Overfitting: model performs well on training data but generalizes poorly on validation data \n",
    "- Underfitting: Poor performance on both \n",
    "\n",
    "### Learning curves  \n",
    "Learning curves are plots of the model’s training error and validation error as a function of the training iteration.  \n",
    "Evaluate the model at regular intervals during training on both the training set and the validation set, and plot the results.  \n",
    "Scikit-Learn has a useful `learning_curve()` function to help with this: it trains and evaluates the model using cross-validation.  \n",
    "By default it retrains the model on growing subsets of the training set.  \n",
    "The function returns the training set sizes at which it evaluated the model, and the training and validation scores it\n",
    "measured for each size and for each cross-validation fold.  \n",
    "See the 2 figures below, which one is underfitting/overfitting? Why?  \n",
    "If there is a gap between the curves, it means that the model performs significantly better on the training data than on the validation data, which is the hallmark of an overfitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "print(len(X), len(y))\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "# learning_curve() returns the scores of the model on the training set and the validation set\n",
    "# the parameter scoring=\"neg_root_mean_squared_error\" means that the function will return the negative RMSE\n",
    "# so we need to negate the scores to get the RMSE\n",
    "# np.linspace(0.01, 1.0, 40) generates 40 evenly spaced training set sizes between 1% and 100% of the full training set size\n",
    "# the full training set size = 4/5 of the total number of instances (80 instances) because of cv=5, which means that we are using 5-fold cross-validation\n",
    "\n",
    "print(train_sizes)\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "# extra code – beautifies and saves Figure 4–15\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([0,80, 0, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is **underfitting**. To see why, first let’s look at the training error. When there\n",
    "are just one or two instances in the training set, the model can fit them perfectly,\n",
    "which is why the curve starts at zero. But as new instances are added to the training\n",
    "set, it becomes impossible for the model to fit the training data perfectly, both because\n",
    "the data is noisy and because it is not linear at all. So the error on the training\n",
    "data goes up until it reaches a plateau, at which point adding new instances to the\n",
    "training set doesn’t make the average error much better or worse. Now let’s look at\n",
    "the validation error. When the model is trained on very few training instances, it is\n",
    "incapable of generalizing properly, which is why the validation error is initially quite\n",
    "large. Then, as the model is shown more training examples, it learns, and thus the\n",
    "validation error slowly goes down. However, once again a straight line cannot do a\n",
    "good job of modeling the data, so the error ends up at a plateau, very close to the\n",
    "other curve.\n",
    "\n",
    "If your model is underfitting the training data, adding more training\n",
    "examples will not help. You need to use a better model or come\n",
    "up with better features.\n",
    "\n",
    "Now let's look at the learning curves for a 10 dimensional polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "# train_sizes=np.linspace(0.01, 1.0, 40) means that the training set size will be 1%, 2%, 3%, ..., 100% of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These learning curves look a bit like the previous ones, but there are two very\n",
    "important differences:\n",
    "- The error on the training data is much lower than before.\n",
    "- There is a gap between the curves. This means that the model performs significantly\n",
    "better on the training data than on the validation data, which is the hallmark of an overfitting model. If you used a much larger training set, however,\n",
    "the two curves would continue to get closer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias/Variance Trade-off\n",
    "Statistics/Machine Learning: a model’s generalization error is the sum of three very different errors:  \n",
    " \n",
    "**Bias:** Wrong assumptions, e.g. linear while it is actually quadratic. A high-bias model is most likely to underfit the training data.\n",
    "\n",
    "**Variance:** \n",
    "Model’s excessive sensitivity to small variations in the training data.  \n",
    "A model with many degrees of freedom (e.g. high-degree polynomial) is likely to have high variance and thus overfit the training data.\n",
    "\n",
    "**Irreducible error:**  \n",
    "Noisiness of the data itself. Reduction only by cleaning up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).  \n",
    "\n",
    "Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
    "Conversely, reducing a model’s complexity increases its bias and reduces its variance => **trade-off**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. \n",
    "- A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.\n",
    "- For a linear model, regularization is typically achieved by constraining the weights of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression is a regularized version of Linear Regression: a regularization term is added to the cost function.\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta_i^2\n",
    "$$\n",
    "\n",
    "The hyperparameter *α* controls how much you want to regularize the model.\n",
    "- If *α = 0*, then Ridge Regression is just Linear Regression. \n",
    "- If *α* is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean (because $θ_1$ to $θ_n$ will tend to zero).\n",
    "\n",
    "Note that the bias term $θ_0$ is not regularized (the sum starts at *i = 1*, not *0*).\n",
    "\n",
    "**WARNING**  \n",
    "It is important to scale the data (e.g., using a StandardScaler) before performing\n",
    "Ridge Regression, as it is sensitive to the scale of the input features. This is true for most regularized models.\n",
    "\n",
    "The figures below show several Ridge models trained on some linear data using different $α$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 20\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1) # 100 evenly spaced numbers in [0, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42) # for now don't worry about the solver parameter\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver=\"sag\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear (left) and a 10th degree polynomial (right) models, both with various levels of\n",
    "ridge regularization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        if polynomial:\n",
    "            model = Pipeline([\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "                    (\"std_scaler\", StandardScaler()),\n",
    "                    (\"regul_reg\", model),\n",
    "                ])\n",
    "        model.fit(X, y)\n",
    "        y_new_regul = model.predict(X_new)\n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 3, 0, 3.5])\n",
    "    plt.grid()\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "plt.subplot(121)\n",
    "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(122)\n",
    "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how increasing $α$ leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing overfitting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Absolute Shrinkage and Selection Operator Regression **(Lasso Regression)** is another regularized version of Linear Regression.  \n",
    "Just like Ridge Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm of the weight vector instead of the square of the ℓ2 norm.  \n",
    "Lasso Regression cost function:\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + 2\\alpha \\sum_{i=1}^{n} |\\theta_i|\n",
    "$$\n",
    "\n",
    "An important characteristic of Lasso Regression is that it **tends to eliminate the weights of the least important features (i.e., set them to zero)**.  \n",
    "For example, the dashed line in the righthand plot below (with α = 0.01) looks roughly cubic: all the weights for the high-degree polynomial features are equal to zero.  \n",
    "In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights). T\n",
    "\n",
    "The figure below shows the same thing as before but replaces Ridge models with Lasso models and uses different α values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# extra code – this cell generates and saves Figure 4–18\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "plt.subplot(121)\n",
    "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.subplot(122)\n",
    "plot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Logistic Regression (also called Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). \n",
    " - If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). \n",
    " - This makes it a binary _classifier_ (although it is a _regression_ algorithm)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Estimating probabilities\n",
    "\n",
    " Just like a Linear Regression model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the logistic of this result.\n",
    "\n",
    " $$\n",
    " \\hat{p} = h_{\\theta}(x) = \\sigma(\\mathbf{x^T}\\mathbf{\\theta})\n",
    " $$\n",
    "\n",
    " where, as we have seen before, \n",
    "\n",
    " $$\n",
    "\\mathbf{x^T}\\mathbf{\\theta} = θ_0x_0 + θ_1x_1 + θ_2x_2 +⋯+ θ_nx_n \n",
    " $$\n",
    " with $x_0 = 1$\n",
    "\n",
    " The logistic—noted $σ(·)$— is a sigmoid function (i.e., S-shaped) that outputs a number between 0 and 1. It is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1 + e^{-t}}\n",
    "$$\n",
    "\n",
    "<img src=\"img/logistic_function_plot.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Logistic Regression model has estimated the probability $\\hat{p} = h_{\\theta}(x)$ that an instance $\\mathbf{x}$ belongs to the positive class, it can make its prediction $ŷ$\n",
    "easily:\n",
    "\n",
    "$\\hat{y} = 0$  if  $\\hat{p} < 0.5$   \n",
    "\n",
    "$\\hat{y} = 1$  if  $\\hat{p} \\ge 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way a logistic regression model is trained is outside the scope of this course but again it comes down to searching the minimum of a cost function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers\n",
    "of three different species: Iris setosa, Iris versicolor, and Iris virginica.  \n",
    " \n",
    "<!-- ![](img/iris.png) -->\n",
    "<img src=\"img/iris.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to build a classifier to detect the Iris virginica type based only on the petal width feature. First let’s load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)  # as_frame=True returns a pandas DataFrame instead of a NumPy array\n",
    "# just like the MNIST dataset, the iris dataset is so popular that Scikit-Learn provides a helper function to load it. \n",
    "# This function returns a Bunch object, similar to a dictionary, containing the data and the target values. \n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "iris.target.head(3)  # note that the instances are not shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s train a Logistic Regression model to predict whether or not it is a virginica (binary classifier) based on the petal width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data[[\"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the model’s **estimated probabilities** for flowers with **petal widths varying from 0 cm to 3 cm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # generate random petal widths and reshape to get a column vector\n",
    "print(X_new[0][0])\n",
    "\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "print(y_proba) # y_proba is a matrix showing probabilities \n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "print(decision_boundary)\n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
    "\n",
    "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
    "plt.legend(loc=\"center left\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y_proba[:, 1] >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X_new[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(decision_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y_predict = log_reg.predict(X_new)\n",
    "y_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the book is actually a bit fancier:  \n",
    "\n",
    "<img src=\"img/logistic_regression_plot.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- The petal width of Iris virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm\n",
    "- The other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. \n",
    "- There is a bit of overlap. \n",
    "- Above about 2 cm the classifier is highly confident that the flower is an Iris virginica (it outputs a high probability for that class). \n",
    "- Below 1 cm it is highly confident that it is not an Iris virginica (high probability for the “Not Iris virginica” class). \n",
    "- In between these extremes, the classifier is unsure. \n",
    "- However, if you ask it to predict the class (using the predict() method rather than the predict_proba() method), it will return whichever class is the most likely.  \n",
    "- Therefore, there is a decision boundary at around 1.6 cm where both probabilities are equal to 50%: if the petal width is higher than 1.6 cm, the classifier will predict that the flower is an Iris virginica, and otherwise it will predict that it is not (even if it is not very confident):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose the case of more than one input feature:\n",
    "- The figure below shows the same dataset, but this time displaying two features: petal width and length. \n",
    "- Once trained, the Logistic Regression classifier can, based on these two features, estimate the probability that a new flower is an Iris virginica. \n",
    "- The dashed line represents the points where the model estimates a 50% probability: this is the model’s decision boundary: it is a linear boundary. \n",
    "- Each parallel line represents the points where the model outputs a specific probability, from 15% (bottom left) to 90% (top right). \n",
    "- All the flowers beyond the top-right line have an over 90% chance of being Iris virginica, according to the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/logistic_regression_contour_plot.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Logistic Regression model can be generalized to **support multiple classes directly**, without having to train and combine multiple binary\n",
    "classifiers (as discussed in Chapter 3). \n",
    "- This is called _Softmax Regression_, or _Multinomial Logistic Regression_.\n",
    "\n",
    "The idea is simple: when given an instance $\\mathbf{x}$, the Softmax Regression model first computes a score $s_k(x)$ for each class $k$, then estimates the probability of\n",
    "each class by applying the _softmax function_.\n",
    "\n",
    "The equation to compute $s_k(x)$ for class $k$ should look familiar, as it is just like the equation for Linear Regression prediction:\n",
    "\n",
    "$$\n",
    "s_k(x) = \\mathbf{x^T}\\mathbf{\\theta}^{(k)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\theta}^{(k)}$ is the dedicated parameter vector for class $k$. \n",
    "\n",
    "- Once you have computed the score of every class for the instance $\\mathbf{x}$, you can estimate the probability $\\hat{p}$ that the instance belongs to class $k$ by running the scores through the softmax function. \n",
    "- This function computes the  exponential of every score, then normalizes them (dividing by the sum of all the exponentials to ensure the probabilities for all class sum up to one):\n",
    "$$\n",
    "\\hat{p}_k = \\mathbf{\\sigma}(\\mathbf{s(x)})_k = \\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(\\mathbf{x}))}\n",
    "$$\n",
    "with:\n",
    "- $K$ the number of classes\n",
    "- $\\mathbf{s(x)}$ a vector containing the scores of each class for the instance $\\mathbf{x}$.\n",
    "- $\\mathbf{\\sigma}(\\mathbf{s(x)}))_k$ the estimated probability that the instance $x$ belongs to class $k$, given the scores of each class for that instance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability (which is\n",
    "simply the class with the highest score).\n",
    "\n",
    "Again the training algorithm itself is outside the scope of this course.\n",
    "\n",
    "Let’s use Softmax Regression to classify the iris flowers into all three classes. \n",
    "- Scikit-Learn’s `LogisticRegression` uses softmax regression automatically when you train it on more than two classes. \n",
    "- Logistic Regression uses regularization by default. The parameter `C` (default 1.0) is the inverse of the regularization strenght and must be a positive float. Smaller values specify stronger regularization. \n",
    "- You could use grid search with cross validation to find the optimal value, but here we choose `C=30` (weak regularization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next time you find an iris with petals that are 5 cm long and 2 cm wide, you can ask your model to tell you what type of iris it is, and it will\n",
    "answer Iris virginica (class 2) with 95.9% probability (or Iris versicolor with 4.1% probability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "softmax_reg.predict([[5,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "softmax_reg.predict_proba([[5,2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the resulting decision boundaries, represented by the background colors. \n",
    "  \n",
    "Observations: \n",
    "- the decision boundaries between any two classes are linear. \n",
    "- the probabilities for the Iris versicolor class are represented by the curved lines (e.g., the line labeled with 0.450 represents the 45% probability boundary).\n",
    "- the model can predict a class that has an estimated probability below 50%. For example, at the point where all decision boundaries meet, all classes have  an equal estimated probability of 33%.  \n",
    "\n",
    "<img src=\"img/softmax_regression_contour_plot.png\" width=\"75%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "6e919314690ba078a9c4cab482e45708028526432cf77d9e4e02ebe923b94e34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
