{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 5 – Support Vector Machines**\n",
    "\n",
    "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing **linear or nonlinear classification, regression, and even outlier detection**. It is one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it in their toolbox. SVMs are particularly well suited **for classification of complex small- or medium-sized datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/jdecorte/machinelearning/blob/main/050-support_vector_machines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We explain SVM by comparing it with Logistic Regression (see previous chapter). \n",
    "- The figure below shows part of the iris dataset. \n",
    "![](img/large_margin_classification_plot.png)  \n",
    "**Observations**:\n",
    "- The two classes can clearly be separated easily with a straight line (they are linearly separable). \n",
    "- The left plot shows the decision boundaries of three possible linear classifiers. \n",
    "- The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. \n",
    "- The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not\n",
    "perform as well on new instances. \n",
    "- In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two\n",
    "classes but also stays as far away from the closest training instances as possible. \n",
    "- You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. \n",
    "- This is called _large margin classification_.\n",
    "- Adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the _support vectors_(they are circled in the figure).\n",
    "\n",
    "**Warning**:\n",
    "SVMs are sensitive to the feature scales: \n",
    "- In the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. \n",
    "- After feature scaling (e.g., using Scikit-Learn’s StandardScaler), the decision boundary in the right plot looks much better.\n",
    "![](img/sensitivity_to_feature_scales_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Margin Classification\n",
    "**Hard Margin Classification**: \n",
    "- We strictly impose that all instances must be off the street and on the right side. \n",
    "- Two main issues: \n",
    "    - it only works if the data is linearly separable.\n",
    "    - it is sensitive to outliers: the figure shows the iris dataset with just one additional outlier. \n",
    "![](img/sensitivity_to_outliers_plot.png)\n",
    "        - on the left, it is impossible to find a hard margin \n",
    "        - on the right (with another outlier), the decision boundary ends up very different from the one we saw above without the outlier, and it will probably not generalize as well (the \"street is too small\")\n",
    "\n",
    "**Soft Margin Classification**: \n",
    "- This is a more flexible model. \n",
    "- The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e.,\n",
    "instances that end up in the middle of the street or even on the wrong side).\n",
    "- In Scikit-Learn you can specify the hyperparameter **C**: \n",
    "    - low: model on the left\n",
    "    - high: model on the right\n",
    "![](img/regularization_plot.png)\n",
    "- Margin violations are bad: it’s usually better to have few of them. \n",
    "- However, in this case the model on the left has a lot of margin violations but will probably generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a\n",
    "linear SVM model (using the LinearSVC class with `C=1` and the hinge loss function,\n",
    "described shortly) to detect Iris virginica flowers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge', random_state=42))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**  \n",
    "- Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "- Without going into further mathematical details the hinge loss function is used as a cost (or loss) function with SVM. It can use Gradient Descent to determine the minimum loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear SVM Classification\n",
    "- Linear SVM classifiers are efficient and work surprisingly well in many cases.\n",
    "- But: many datasets are not even close to being linearly separable.\n",
    "- One approach to handling nonlinear datasets is to add more features, such as polynomial features (see Chapter 4).\n",
    "- In some cases this can result in a linearly separable dataset.\n",
    "- Example: add $x_2 = (x_1)^2$ as extra feature -> the resulting 2D dataset is now perfectly linearly separable: \n",
    "\n",
    "![](img/higher_dimensions_plot.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this idea using Scikit-Learn: create a Pipeline\n",
    "- PolynomialFeatures transformer (see Chapter 4)\n",
    "- StandardScaler\n",
    "- LinearSVC\n",
    "\n",
    "We test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcor864\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge', random_state=42))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prove that this model creates the following separation between the two \"moon\" classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/moons_polynomial_svc_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs). \n",
    "- At a **low polynomial degre**e, this method **cannot deal with very complex datasets**.\n",
    "- With a **high polynomial degree** it creates a huge number of features, making the **model too slow**.\n",
    "- Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the _kernel trick_ (explained in a moment). \n",
    "    - **Get the same result** as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. \n",
    "    - There is **no combinatorial explosion of the number of features** because you don’t actually add any features. \n",
    "    - This trick is **implemented by the SVC class**. \n",
    "    \n",
    "Let’s test it on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code trains an SVM classifier using a 3d-degree polynomial kernel. It is represented on the left in the figure below. \n",
    "- The code below does the same but now for a 10th-degree polynomial. It is represented on the right in the figure. \n",
    "- Hyperparameter `coef0` controls how much the model is influenced by high-degree terms versus low-degree terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=100, degree=10, kernel='poly'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly100_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n",
    "    ])\n",
    "poly100_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/moons_kernelized_polynomial_svc_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**\n",
    "\n",
    "- A common approach to finding the right hyperparameter values is to use grid search (see Chapter 2). \n",
    "- It is often faster to first do a very coarse grid search, then a finer grid search around the best values found.\n",
    "- Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "\n",
    "- LinearSVC: \n",
    "    - does not support the kernel trick\n",
    "    - scales almost linear with the number of training instances and the number of features: training time $\\sim O(m \\times n)$.\n",
    "    - takes longer if you require very high precision: controlled by the tolerance hyperparameter $ϵ$ (epsilon, called `tol` in Scikit-Learn). In most classification tasks, the\n",
    "default tolerance is fine. \n",
    "- SVC class: \n",
    "    - supports kernel trick\n",
    "    - training time complexity is usually between $O(m^2 \\times n)$ and $O(m^3 \\times n)$\n",
    "    - very slow when number of training instances gets large (e.g., hundreds of thousands of instances).\n",
    "    - perfect for complex small or medium-sized training sets.\n",
    "    - scales well with the number of features, especially with _sparse_ features (i.e., when each instance has few nonzero features): scales roughly with the average number of nonzero features per\n",
    "instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Regression\n",
    "To use SVMs for regression instead of classification, the trick is to reverse the objective: \n",
    "- instead of trying to fit the largest possible street between two classes while limiting margin violations\n",
    "- tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street).\n",
    "- width of the street is controlled by a hyperparameter, $ϵ$ (epsilon).\n",
    "- The figure shows two linear SVM Regression models trained on some random linear data, one with a large margin ($ϵ$ = 1.5) and the other with a small margin ($ϵ$ = 0.5).\n",
    "![](img/svm_regression_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces the model represented on the left in the figure above (the training data should be scaled and centered first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To tackle **nonlinear regression tasks**, you can use a kernelized SVM model. \n",
    "- The figure below shows SVM Regression on a random quadratic training set, using a second-degree polynomial kernel. \n",
    "- There is little regularization in the left plot (i.e., a large  `C` value), and much more regularization in the right plot (i.e., a small `C` value).\n",
    "![](img/svm_with_polynomial_kernel_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses Scikit-Learn’s SVR class (which supports the kernel trick) to produce the model represented on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Complexity\n",
    "- The `LinearSVR` class scales linearly with the size of the training set (just like the LinearSVC class)\n",
    "- The `SVR` class gets much too slow when the training set grows large (just like the SVC class)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
